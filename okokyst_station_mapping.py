import numpy as np
import pandas as pd
import datetime

__author__   = 'Elizaveta Protsenko'
__email__    = 'Elizaveta.Protsenko@niva.no'
__created__  = datetime.datetime(2020, 9, 23)
__version__  = "1.0"
__status__   = "Development"


hardanger_stations = pd.DataFrame({
    "st_names": ['VT53', 'VT69', 'VT70', 'VT74'],
    "st_longnames": ['Tveitneset', 'Korsfjorden', 'Bjornafjorden','Maurangsfjorden'],
    "st_lat": ['60.4014', '60.1788', '60.1043', '60.1061'],
    "st_long": ['6.4398', '5.2393', '5.4742', '6.168'],
    "depths":    [700.0, 5.0, 590.0, 230.0]
    })

sognefjorden_stations = pd.DataFrame({
    "st_names":     ['VT16','VT79'],
    "st_longnames": ['Kyrkjeboe','Naersnes'],
    "st_lat":       ['61.14600', '60.9963'],
    "st_long":      [5.9527, 7.0556],
    "depths":       [1300.0, 500.0]
    })

stations_dict = {'hardanger': hardanger_stations,
                 'sognefjorden': sognefjorden_stations}


# read the document and skip undefined number of unneeded rows
def read_convert_df(base_path, file_path):
    path = base_path + file_path
    print (path)
    for n in range(0, 10):
        try:
            df_all = pd.read_csv(path, skiprows=n, header=n-1, sep=';', decimal=',')
            break
        except:
            pass
            df_all = None

    if not df_all is None:
        # rename columns
        try:
            df_all = df_all.rename(columns={'Press': "Depth"})
        except:
            df_all = df_all.rename(columns={'Depth(u)': "Depth"})

        # Change datatypes to float
        convert_dict = {
            'Depth': float
        }
        df_all = df_all.astype(convert_dict)
        df_all = df_all.dropna(how='all', axis=1)
        df_all = df_all.round(4)
        return df_all
    else:
        print('could not read the file')


def match_stations_by_depth(group, region, base_path):

    # Find the max depth in each group
    max_depth = np.max(group['Depth'])

    # Get number of the cast
    Ser = group.Ser.values[0]

    stations_info = stations_dict[region]

    # find the closest depth in the arr with all stations for this region

    difs = stations_info['depths'].values - max_depth
    print ('difs', difs)
    difs = difs[np.where(difs > 0)]
    print ('difs positive', difs)
    min_dif = min(difs)
    print (min_dif)
    if min_dif < 25:

        # Find the station with the closest depth
        nearest_depth_id = np.where(difs == min_dif)[0][0]
        df_matched_st = stations_info.where(
                stations_info['depths'] == stations_info['depths'].values[nearest_depth_id]).dropna()
        station_name = df_matched_st['st_names'].values[0]
        
        print (' ')
        print('max depth', max_depth)
        print('min difference', min_dif)
        print(station_name)

        filename = base_path + "\\" + station_name + '_autogenerated.txt'
    else:
        print (' ')
        print ('Was not able to find a matching station name')
        print ('min difference', min_dif)
        print ('max depth is ', max_depth)

        filename = base_path + r'\\Unknown_station' + str(Ser) + '_autogenerated.txt'

    group.to_csv(filename, index=False, sep = ';')

def process_station(base_path, file_path,region):
    df_all = read_convert_df(base_path, file_path)
    df_all.groupby('Ser').apply(match_stations_by_depth,  base_path=base_path, region=region)


if __name__ == "__main__":
    work_dir = r"C:\Users\ELP\PycharmProjects\okokyst_split_files.py\\"


    #process_station(base_path=work_dir+r"\2017-02-20\2017-02-20 CTD data\\",
    #                file_path=r'2017-02-20 hardanger_salt.txt',
    #                region='hardanger')

    #process_station(base_path=work_dir + r'2018-03-13 CTD data',
    #                file_path=r'\O-18075_20180313_Leon.txt',
    #                region='sognefjorden')


    process_station(base_path=work_dir + r'2018-04-15 CTD data',
                    file_path=r"\\O-18075_20180415_Leon.txt",
                    region='sognefjorden')
